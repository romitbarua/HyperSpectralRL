{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78720978",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/romitbarua/opt/anaconda3/envs/cs285/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "import h5py\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import accuracy_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import torch.nn as nn\n",
    "from scipy.stats.stats import pearsonr\n",
    "import torch\n",
    "from matplotlib.pyplot import figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4cf4d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DataManager():\n",
    "    def __init__(self, params, num_bands):\n",
    "        self.rl_data = None\n",
    "        self.dataset_type = params['dataset_type']\n",
    "        self.data_file_path = params['data_file_path']\n",
    "        self.sample_ratio = params['sample_ratio']\n",
    "        \n",
    "        self.num_bands = num_bands\n",
    "        #load the data\n",
    "        assert self.dataset_type in ('IndianPines', 'Botswana', 'SalientObjects'), f'{self.dataset_type} is not valid'\n",
    "        #separating out in case any of the data requires unique pre-processig\n",
    "        if self.dataset_type == 'IndianPines':\n",
    "            self.load_indian_pine_data()\n",
    "        elif self.dataset_type == 'Botswana':\n",
    "            self.load_botswana_data()\n",
    "        elif self.dataset_type == 'SalientObjects':\n",
    "            self.load_salient_objects_data()\n",
    "        #self.x_train = None\n",
    "        #self.y_train = None\n",
    "        #self.x_test = None\n",
    "        #self.y_test = None\n",
    "        \n",
    "    def load_indian_pine_data(self):\n",
    "        hyper_path = '../data/indian_pines/hyperspectral_imagery/indian_pines_corrected.npy'\n",
    "        hyper = np.load(hyper_path)\n",
    "        print(hyper.shape)\n",
    "        # randomly sample for x% of the pixels\n",
    "        indices = np.random.randint(0, hyper.shape[0], int(hyper.shape[0]*self.sample_ratio))\n",
    "        self.rl_data = hyper[indices, :]\n",
    "        print(self.rl_data.shape)\n",
    "        \n",
    "    def load_salient_objects_data(self):\n",
    "        hyper_path = '../data/salient_objects/hyperspectral_imagery/0001.npy'\n",
    "        hyper = np.load(hyper_path)\n",
    "        print(hyper.shape)\n",
    "        # randomly sample for x% of the pixels\n",
    "        indices = np.random.randint(0, hyper.shape[0], int(hyper.shape[0]*self.sample_ratio))\n",
    "        self.rl_data = hyper[indices, :]\n",
    "        print(self.rl_data.shape)\n",
    "        \n",
    "    def load_botswana_data(self):\n",
    "        self.rl_data = scipy.io.loadmat(self.data_file_path)\n",
    "    #def load_salient_objects(self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "433ba101",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer():\n",
    "    \n",
    "    def __init__(self, size=10000):\n",
    "        self.size = size\n",
    "        self.paths = []\n",
    "        \n",
    "    def add_trajectories(self, paths):\n",
    "        self.paths.extend(paths)\n",
    "        self.paths = self.paths[-self.size:]\n",
    "        \n",
    "    def sample_buffer_random(self, num_trajectories):\n",
    "        \n",
    "        rand_idx = np.random.permutation(len(self.paths))[:num_trajectories]\n",
    "        return [self.paths[i] for i in rand_idx]\n",
    "        #return self.paths[rand_idx]\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c496803",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    \n",
    "    def __init__(self, params):\n",
    "        \n",
    "        self.agent_params = params['agent']\n",
    "        self.num_bands = self.agent_params['num_bands']\n",
    "        self.n_iter = self.agent_params['n_iter']\n",
    "        self.trajectory_sample_size = self.agent_params['trajectory_sample_size']\n",
    "        self.batch_size = self.agent_params['batch_size']\n",
    "        self.num_critic_updates = self.agent_params['num_critic_updates']\n",
    "        \n",
    "        valid_rewards = ['correlation', 'mutual_info']\n",
    "        assert self.agent_params['reward_type'] in valid_rewards, 'rewards must be one of ' + valid_rewards.join(',') \n",
    "        \n",
    "        if self.agent_params['reward_type'] == 'correlation':\n",
    "            self.reward_func = self.calculate_correlations\n",
    "        elif self.agent_param['reward_type'] == 'mutual_info':\n",
    "            self.reward_func = self.calculate_mutual_infos\n",
    "        \n",
    "        self.data_params = params['data']\n",
    "        self.DataManager = DataManager(self.data_params, self.num_bands)\n",
    "        self.band_selection_num = self.data_params['band_selection_num']\n",
    "\n",
    "        self.critic_params = params['critic']\n",
    "        self.critic = QCritic(self.critic_params, self.num_bands)\n",
    "        \n",
    "        \n",
    "        self.policy_params = params['policy']\n",
    "        self.policy = ArgMaxPolicy(self.policy_params, self.critic)\n",
    "        \n",
    "        self.replay_buffer = ReplayBuffer()\n",
    "        \n",
    "        self.cache = {}\n",
    "        \n",
    "        self.logging_df = pd.DataFrame()\n",
    "        \n",
    "    \n",
    "    def generateTrajectories(self):\n",
    "        \n",
    "        #we expect paths to be a list of trajectories\n",
    "        #a trajectory is a list of Path objects\n",
    "        paths = []\n",
    "        for i in range(self.trajectory_sample_size):\n",
    "            \n",
    "            path = self.sampleTrajectory()\n",
    "#             print(f'Iter {i}')\n",
    "#             print([p['re'] for p in path])\n",
    "            \n",
    "            paths.append(path)\n",
    "    \n",
    "        return paths\n",
    "    \n",
    "    def sampleTrajectory(self, iter_num = 1):\n",
    "            \n",
    "        #select 30 actions\n",
    "        state = np.zeros(self.num_bands)\n",
    "        state_next = state.copy()\n",
    "        \n",
    "        #paths will be a list of dictionaries\n",
    "        path = []\n",
    "        for i in range(self.band_selection_num):\n",
    "            \n",
    "            action, action_type = self.policy.get_action(state)\n",
    "            state_next[action] += 1\n",
    "\n",
    "            reward, correlation_current_state, correlation_next_state = self.calculate_reward(state, state_next)\n",
    "\n",
    "            terminal = 1 if i == self.band_selection_num - 1 else 0\n",
    "            path.append(self.Path(state.copy(), action, state_next.copy(), reward, terminal))\n",
    "            \n",
    "            state = state_next.copy()\n",
    "        \n",
    "            if iter_num % 100 == 0:\n",
    "                print(\"Iter : \", iter_num)\n",
    "                q_values = self.critic.get_action(state)\n",
    "                \n",
    "                sampled_paths = self.replay_buffer.sample_buffer_random(1)\n",
    "                \n",
    "                flat_sampled_path = [path for trajectory in sampled_paths for path in trajectory]\n",
    "                obs = np.array([path['ob'] for path in flat_sampled_path])\n",
    "                acs = np.array([path['ac'] for path in flat_sampled_path])\n",
    "                obs_next = np.array([path['ob_next'] for path in flat_sampled_path])\n",
    "                res = np.array([path['re'] for path in flat_sampled_path])\n",
    "                terminals = np.array([path['terminal'] for path in flat_sampled_path])\n",
    "                \n",
    "                loss_value = self.critic.update(obs, acs, obs_next, res, terminals)\n",
    "                \n",
    "                row = {\n",
    "                    \"iter_num\": iter_num,\n",
    "                    \"Selected Band\": i,\n",
    "                    \"Action Type\": action_type,\n",
    "                    \"Mean\": torch.mean(q_values).detach().numpy(),\n",
    "                    \"Min\": torch.min(q_values).detach().numpy(),\n",
    "                    \"Max\": torch.max(q_values).detach().numpy(),\n",
    "                    \"Correlation Current State\" : correlation_current_state,\n",
    "                    \"Correlation Next State\" : correlation_next_state,\n",
    "                    \"Reward\" : reward,\n",
    "                    \"Loss\" : loss_value\n",
    "                }\n",
    "                \n",
    "                self.logging_df = self.logging_df.append(row, ignore_index=True)\n",
    "                \n",
    "                \n",
    "#                 print(self.logging_df)\n",
    "        \n",
    "        #path returns state, action, state_next, reward, terminal\n",
    "        return path\n",
    "                   \n",
    "        \n",
    "    def runAgent(self):\n",
    "        \n",
    "        for iter_num in range(self.n_iter):\n",
    "            \n",
    "            print('Iteration ', iter_num, ':')\n",
    "            \n",
    "            paths = self.generateTrajectories()\n",
    "            self.replay_buffer.add_trajectories(paths)\n",
    "            \n",
    "            for _ in range(self.num_critic_updates):\n",
    "                sampled_paths = self.replay_buffer.sample_buffer_random(self.agent_params['batch_size'])\n",
    "                \n",
    "                flat_sampled_path = [path for trajectory in sampled_paths for path in trajectory]\n",
    "                obs = np.array([path['ob'] for path in flat_sampled_path])\n",
    "                acs = np.array([path['ac'] for path in flat_sampled_path])\n",
    "                obs_next = np.array([path['ob_next'] for path in flat_sampled_path])\n",
    "                res = np.array([path['re'] for path in flat_sampled_path])\n",
    "                terminals = np.array([path['terminal'] for path in flat_sampled_path])\n",
    "                \n",
    "                critic_loss = self.critic.update(obs, acs, obs_next, res, terminals)\n",
    "                \n",
    "            self.critic.update_target_network()\n",
    "            \n",
    "            #sample a single trajectory\n",
    "            print('------------------------------------EVAL Results------------------------------')\n",
    "            eval_path = self.sampleTrajectory(iter_num)\n",
    "#             print(self.cache)\n",
    "            #print(eval_path[-1])\n",
    "            print('Selected_Bands: ', np.argwhere(eval_path[-1]['ob_next']>0))\n",
    "            print('Num_Selected_Bands: ', np.argwhere(eval_path[-1]['ob_next']>0).shape[0])\n",
    "            print('Eval_Return: ', np.sum(eval_path[-1]['re']))\n",
    "            print('Critic_Loss: ', critic_loss)\n",
    "                \n",
    "\n",
    "    def calculate_reward(self, state, state_next):\n",
    "        #for future, save down the previous state so that we can avoid a calc\n",
    "        \n",
    "#         print(list(np.argwhere(np.array(state) != 0)), list(np.argwhere(np.array(state_next) != 0)))\n",
    "        if list(np.argwhere(np.array(state) != 0)) == list(np.argwhere(np.array(state_next) != 0)):\n",
    "#             print(\"same action selected\")\n",
    "            return -1, \"Indef\", \"Indef\"\n",
    "        else:\n",
    "        \n",
    "            a = self.reward_func(state)\n",
    "            b = self.reward_func(state_next)\n",
    "            return a-b, a, b\n",
    "    \n",
    "    \n",
    "    def calculate_correlations(self, state):\n",
    "        \n",
    "#         if repr(state) in self.cache:\n",
    "#             return self.cache[repr(state)]\n",
    "        \n",
    "        #deal with the first state\n",
    "        ##### THIS LOGIC SEEMS WRONG - REGARDLESS OF THE FIRST PICK, YOU HAVE A REWARD OF 0#####\n",
    "        if np.sum(state) <= 1:\n",
    "            return 0\n",
    "        \n",
    "        selected_bands = []\n",
    "        non_zero_bands = np.argwhere(np.array(state) != 0)\n",
    "        for band in non_zero_bands:\n",
    "#             print(band[0])\n",
    "            selected_bands.extend([band[0]]*int(state[band[0]]))\n",
    "        #print(selected_bands)\n",
    "        #selected_bands = np.squeeze(np.argwhere(np.array(state)==1))\n",
    "        corr_sum = 0\n",
    "        for idx_i, i in enumerate(selected_bands):\n",
    "            for idx_j, j in enumerate(selected_bands):\n",
    "                if idx_i != idx_j:\n",
    "                    \n",
    "                    if repr((i,j)) in self.cache:\n",
    "                        result = self.cache[repr((i,j))]\n",
    "                    else:\n",
    "                        result = abs(pearsonr(self.DataManager.rl_data[:, i], self.DataManager.rl_data[:, j])[0])\n",
    "                        self.cache[repr((i,j))] = result\n",
    "                    \n",
    "                    corr_sum += result\n",
    "                    \n",
    "#                     corr_sum += abs(pearsonr(self.DataManager.rl_data[:, i], self.DataManager.rl_data[:, j])[0])\n",
    "        \n",
    "#         self.cache[repr(state)] = corr_sum/(len(selected_bands)**2)\n",
    "        \n",
    "#         return self.cache[repr(state)]\n",
    "        return corr_sum/(len(selected_bands)**2)\n",
    "\n",
    "    def calculate_mutual_infos(self, state):\n",
    "    \n",
    "        selected_bands = []\n",
    "        non_zero_bands = np.argwhere(np.array(state) != 0)\n",
    "        for band in non_zero_bands:\n",
    "            selected_bands.extend([band[0]]*int(state[band[0]]))\n",
    "    \n",
    "        normalized_mutual_info_score_sum = 0\n",
    "        for i in selected_bands:\n",
    "            for j in selected_bands:\n",
    "                \n",
    "                if i != j:\n",
    "\n",
    "                    normalized_mutual_info_score_sum += normalized_mutual_info_score(self.DataManager.rl_data[:, i],\n",
    "                                                                                     self.DataManager.rl_data[:, j])\n",
    "\n",
    "        return normalized_mutual_info_score_sum/(len(selected_bands)**2)\n",
    "\n",
    "            \n",
    "    def Path(self, ob, ac, ob_next, re, terminal):\n",
    "        return {'ob':ob,\n",
    "                'ac':ac,\n",
    "                'ob_next':ob_next,\n",
    "                're':re,\n",
    "                'terminal':terminal\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb12fe7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArgMaxPolicy():\n",
    "    \n",
    "    def __init__(self, params, critic):\n",
    "        self.epsilon = params['epsilon']\n",
    "        self.epsilon_decay = params['epsilon_decay']\n",
    "        self.critic = critic\n",
    "        \n",
    "    def get_action(self, obs):\n",
    "        \n",
    "        q_value_estimates = self.critic.get_action(obs)\n",
    "        unselected_bands = np.squeeze(np.argwhere(obs == 0))\n",
    "        #print(obs)\n",
    "#         print('Predicted Q-Values:', q_value_estimates)\n",
    "        \n",
    "        rand = np.random.rand()\n",
    "        if rand < self.epsilon:\n",
    "            #select a random action\n",
    "#             print('Selected Random')\n",
    "            unselected_bands = np.squeeze(np.argwhere(obs == 0))\n",
    "            selected_idx = np.random.choice(unselected_bands)\n",
    "            action_type = \"Random Action\"\n",
    "\n",
    "        else:\n",
    "#           print('Selected Max')\n",
    "            #q_value_estimates_idx = torch.argsort(q_value_estimates, dim=1)\n",
    "            #q_value_estimates = q_value_estimates[unselected_bands, :]\n",
    "        \n",
    "            #q_filter = q_value_estimates[unselected_bands]\n",
    "            q_value_estimates_idx = torch.argsort(q_value_estimates, descending=True)\n",
    "            q_value_estimates_idx = q_value_estimates_idx[torch.isin(q_value_estimates_idx, torch.tensor(unselected_bands))]\n",
    "            selected_idx = q_value_estimates_idx[0].item()\n",
    "                \n",
    "            #print('Obs ', np.sum(obs))\n",
    "            #print('Shape Q_Filter ', q_filter.shape)\n",
    "            #print('Shape Unselected ', unselected_bands.shape)\n",
    "                \n",
    "            #selected_idx = torch.argmax(q_filter).item()\n",
    "            action_type = \"Max Action\"\n",
    "\n",
    "            \n",
    "        self.decay_epsilon()\n",
    "        return selected_idx, action_type\n",
    "                \n",
    "    def decay_epsilon(self):\n",
    "        self.epsilon *= self.epsilon_decay "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4bcd7fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QCritic():\n",
    "    \n",
    "    def __init__(self, params, num_bands):\n",
    "        \n",
    "        self.num_bands = num_bands\n",
    "\n",
    "        \n",
    "        self.critic = self.create_network()\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(),lr=0.005)\n",
    "\n",
    "        self.critic_target = self.create_network()\n",
    "        \n",
    "        self.gamma = params['gamma']\n",
    "        \n",
    "        self.loss = nn.SmoothL1Loss()\n",
    "    \n",
    "    def create_network(self):\n",
    "        \n",
    "        q_net  = nn.Sequential(\n",
    "        nn.Linear(self.num_bands, self.num_bands*2),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(self.num_bands*2, self.num_bands*2),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(self.num_bands*2, self.num_bands)\n",
    "        )\n",
    "        \n",
    "        return q_net\n",
    "    \n",
    "        \n",
    "    def forward(self, obs):\n",
    "        # will take in one hot encoded states and output a list of qu values\n",
    "        \n",
    "        q_values = self.critic(obs)\n",
    "        \n",
    "        return q_values\n",
    "    \n",
    "    def get_action(self, obs):\n",
    "        \n",
    "        if isinstance(obs, np.ndarray):\n",
    "            obs = from_numpy(obs)\n",
    "            \n",
    "        return self.critic(obs)\n",
    "    \n",
    "    def update(self, obs, ac_n, next_obs, reward_n, terminals):\n",
    "        \n",
    "        obs = self.check_tensor(obs) #comes in as shape \n",
    "        ac_n = self.check_tensor(ac_n)\n",
    "        next_obs = self.check_tensor(next_obs)\n",
    "        reward_n = self.check_tensor(reward_n)\n",
    "        terminals = self.check_tensor(terminals)\n",
    "        \n",
    "        full_q_values = self.critic(obs)\n",
    "        q_actions = full_q_values.argmax(dim=1)\n",
    "        q_values = torch.gather(full_q_values, 1, q_actions.unsqueeze(1)).squeeze(1)\n",
    "        \n",
    "        \n",
    "        #print('Obs ', obs.shape)\n",
    "        #print('Full Q ', full_q_values.shape)\n",
    "        #print('Q Actions ', q_actions.shape)\n",
    "        #print('Q Val ', q_values.shape)\n",
    "        \n",
    "        full_q_next_target = self.critic_target(next_obs)\n",
    "        q_actions_next = self.critic(next_obs).argmax(dim=1)\n",
    "        #q_values_next = full_q_next.max(dim=1)\n",
    "        #print('q_values_next', q_values_next)\n",
    "        q_values_next = torch.gather(full_q_next_target, 1, q_actions_next.unsqueeze(1)).squeeze(1)\n",
    "        \n",
    "        #print('reward', type(reward_n.shape))\n",
    "        #print('q_values_next', type(q_values_next))\n",
    "        #print('terminals', type(terminals))\n",
    "        #print('gamma', type(self.gamma))\n",
    "        target = reward_n + self.gamma*q_values_next*(1-terminals)\n",
    "        target = target.detach()\n",
    "        \n",
    "        #print(f'Target Dim: {target.shape}')\n",
    "        #print(f'Q_Values Dim: {q_values.shape}')\n",
    "        loss = self.loss(q_values, target)\n",
    "        \n",
    "        self.critic_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "        \n",
    "        return loss.item()\n",
    "    \n",
    "    def check_tensor(self, ar):\n",
    "        \n",
    "        if isinstance(ar, np.ndarray):\n",
    "            ar = from_numpy(ar)\n",
    "            \n",
    "        return ar\n",
    "    \n",
    "    def update_target_network(self):\n",
    "        for target_param, param in zip(\n",
    "            self.critic_target.parameters(), self.critic.parameters()):\n",
    "            target_param.data.copy_(param.data)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "770adc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#utility functions\n",
    "## taken from Prof. Sergey Levine's CS285 HW\n",
    "\n",
    "device = 'cpu'\n",
    "\n",
    "def from_numpy(*args, **kwargs):\n",
    "    return torch.from_numpy(*args, **kwargs).float().to(device)\n",
    "\n",
    "def to_numpy(tensor):\n",
    "    return tensor.to('cpu').detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6dbabbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'agent':{\n",
    "            'n_iter':5000,\n",
    "            'trajectory_sample_size': 10,\n",
    "            'batch_size':10,\n",
    "            'num_critic_updates':10,\n",
    "            'num_bands':200,\n",
    "            'reward_type':'correlation'\n",
    "            },\n",
    "          'data':{\n",
    "            'band_selection_num':30,\n",
    "            'dataset_type':'IndianPines',\n",
    "            'data_file_path':r'/Users/romitbarua/Documents/Berkeley/Fall 2022/CS285-Deep Reinforcement Learning/HyperSpectralRL/data/data_indian_pines_drl.mat',\n",
    "            'sample_ratio':0.1\n",
    "            },\n",
    "          'critic':{\n",
    "            'gamma':0.99\n",
    "            },\n",
    "          'policy':{\n",
    "            'epsilon':0,\n",
    "            'epsilon_decay':0.999\n",
    "            }\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e094aa21",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/indian_pines/hyperspectral_imagery/indian_pines_corrected.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/nf/mymsq1x11ls4v86t0_q5v_yw0000gn/T/ipykernel_63856/1252695598.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/nf/mymsq1x11ls4v86t0_q5v_yw0000gn/T/ipykernel_63856/4191642221.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, params)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataManager\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataManager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_bands\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mband_selection_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'band_selection_num'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/nf/mymsq1x11ls4v86t0_q5v_yw0000gn/T/ipykernel_63856/2521380303.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, params, num_bands)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;31m#separating out in case any of the data requires unique pre-processig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'IndianPines'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_indian_pine_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'Botswana'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_botswana_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/nf/mymsq1x11ls4v86t0_q5v_yw0000gn/T/ipykernel_63856/2521380303.py\u001b[0m in \u001b[0;36mload_indian_pine_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_indian_pine_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mhyper_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'../data/indian_pines/hyperspectral_imagery/indian_pines_corrected.npy'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mhyper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhyper_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhyper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m# randomly sample for x% of the pixels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/cs285/lib/python3.7/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    415\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 417\u001b[0;31m             \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menter_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    418\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/indian_pines/hyperspectral_imagery/indian_pines_corrected.npy'"
     ]
    }
   ],
   "source": [
    "agent = Agent(params)\n",
    "agent.runAgent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e36347",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d738c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.logging_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6746d55b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11985f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_df = agent.logging_df[agent.logging_df['Selected Band']==29.0]\n",
    "sns.lineplot(x='iter_num', y='Correlation Next State', data=filter_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4510dd7e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(30):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    plot_df = agent.logging_df[agent.logging_df[\"Selected Band\"] == i]\n",
    "    ax1.plot(plot_df[\"iter_num\"], plot_df[\"Mean\"], color='red', label=\"Mean\")\n",
    "    ax1.plot(plot_df[\"iter_num\"], plot_df[\"Max\"], color='blue', label=\"Max\")\n",
    "    ax1.plot(plot_df[\"iter_num\"], plot_df[\"Min\"], color='green', label=\"Min\")\n",
    "    ax1.axhline(plot_df['Reward'].mean(), color='red')\n",
    "    ax1.set_title(f'Band Selection {i}')\n",
    "    ax1.legend()\n",
    "    ax2.plot(plot_df[\"iter_num\"], plot_df[\"Loss\"], color='red')\n",
    "    ax2.set_title('Loss Function')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9ea8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.logging_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5266626c",
   "metadata": {},
   "outputs": [],
   "source": [
    "band_1_logging_df = agent.logging_df[agent.logging_df[\"Selected Band\"] == 1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39cf17e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "band_1_logging_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0364bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(num = 3, figsize=(8, 5))\n",
    "plt.plot(band_1_logging_df[\"iter_num\"], band_1_logging_df[\"Mean\"], color='red', label=\"Mean\")\n",
    "plt.plot(band_1_logging_df[\"iter_num\"], band_1_logging_df[\"Max\"], color='blue', label=\"Max\")\n",
    "plt.plot(band_1_logging_df[\"iter_num\"], band_1_logging_df[\"Min\"], color='green', label=\"Min\")\n",
    "\n",
    "plt.axhline(band_1_logging_df['Reward'].mean( ))\n",
    "\n",
    "plt.xlabel(\"iter num\")\n",
    "plt.ylabel(\"Q values\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b01749",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(num = 3, figsize=(8, 5))\n",
    "plt.plot(band_1_logging_df[\"iter_num\"], band_1_logging_df[\"Loss\"], color='red')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8c5ec9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab703265",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/romitbarua/Documents/Berkeley/Fall 2022/CS285-Deep Reinforcement Learning/HyperSpectralRL/data/data_indian_pines_drl.mat'\n",
    "data = scipy.io.loadmat(path)['x']\n",
    "band_1 = data[:, 173]\n",
    "band_2 = data[:, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d444156",
   "metadata": {},
   "outputs": [],
   "source": [
    "num = np.sum((band_1 - np.mean(band_1))*(band_2 - np.mean(band_2)))\n",
    "den = np.sqrt(np.sum((band_1 - np.mean(band_1))**2)*np.sum((band_2 - np.mean(band_2))**2))\n",
    "num/den"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d935b982",
   "metadata": {},
   "outputs": [],
   "source": [
    "band_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb388942",
   "metadata": {},
   "outputs": [],
   "source": [
    "band_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27892760",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = []\n",
    "band_1 = data[:, 40]\n",
    "for i in range(2, 200):\n",
    "\n",
    "    band_2 = data[:, i]\n",
    "    num = np.sum((band_1 - np.mean(band_1))*(band_2 - np.mean(band_2)))\n",
    "    den = np.sqrt(np.sum((band_1 - np.mean(band_1))**2)*np.sum((band_2 - np.mean(band_2))**2))\n",
    "    corr.append(num/den)\n",
    "    \n",
    "    \n",
    "sns.histplot(corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2c9baa",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214bdf2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'/Users/romitbarua/Documents/Berkeley/Fall 2022/CS285-Deep Reinforcement Learning/HyperSpectralRL/data/data_indian_pines_drl.mat'\n",
    "num_iter = 50\n",
    "num_bands = 200\n",
    "bands_to_select = 30\n",
    "data = scipy.io.loadmat(path)['x'][:, :num_bands]\n",
    "\n",
    "all_band_selection = []\n",
    "all_corr = []\n",
    "all_reward = []\n",
    "\n",
    "for num_iter in range(num_iter):\n",
    "    selected_bands = list(np.random.choice(np.arange(0, num_bands, 1), 1))\n",
    "    corr = [0]\n",
    "    rewards = [0]\n",
    "    for i in range(bands_to_select):\n",
    "\n",
    "        selected_bands.extend(list(np.random.choice(np.arange(0, num_bands, 1), 1)))\n",
    "        corr_sum = 0\n",
    "        \n",
    "        for idx_i, i in enumerate(selected_bands):\n",
    "            for idx_j, j in enumerate(selected_bands):\n",
    "                if idx_i != idx_j:\n",
    "                    corr_sum += abs(pearsonr(data[:, i], data[:, j])[0])\n",
    "        corr.append(corr_sum/(len(selected_bands)**2))\n",
    "        rewards.append(corr[-2] - corr[-1])\n",
    "    \n",
    "    if num_iter % 100 == 0:\n",
    "        print(num_iter)\n",
    "        \n",
    "    all_band_selection.append(selected_bands)\n",
    "    all_corr.append(corr)\n",
    "    all_reward.append(rewards)\n",
    "    \n",
    "all_band_selection = np.array(all_band_selection)\n",
    "all_corr = np.array(all_corr)\n",
    "all_reward = np.array(all_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78360550",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4eeb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iter = 1000\n",
    "cum_reward = np.zeros((num_iter, bands_to_select))\n",
    "for i in range(bands_to_select):\n",
    "    cum_reward[:, i] = np.sum(all_reward[:, i+1:], axis=1)\n",
    "    \n",
    "# reasonbleness check to make sure this works\n",
    "#print(all_reward[0, :])\n",
    "#print(sum(all_reward[0, :]))\n",
    "#print(cum_reward[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ea32ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(bands_to_select):\n",
    "    sns.histplot(cum_reward[:,i])\n",
    "    plt.title(f'Distribution of Q-Values at band selection {i+1}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177c51fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# reward functions\n",
    "\n",
    "def calculate_correlations(data, num_bands_originally, num_bands_kept):\n",
    "    \n",
    "    selected_bands = np.random.randint(0,num_bands_originally,num_bands_kept)\n",
    "    corr_sum = 0\n",
    "    for i in selected_bands:\n",
    "        for j in selected_bands:\n",
    "            if i != j:\n",
    "            \n",
    "                corr_sum += np.abs(pearsonr(data[:, i], \n",
    "                                 data[:, j])[0])\n",
    "            \n",
    "    return corr_sum/(len(selected_bands)**2)\n",
    "\n",
    "\n",
    "def calculate_mutual_infos(data, num_bands_originally, num_bands_kept):\n",
    "    \n",
    "    selected_bands = np.random.randint(0,num_bands_originally,num_bands_kept)\n",
    "    normalized_mutual_info_score_sum = 0\n",
    "    for i in selected_bands:\n",
    "        for j in selected_bands:\n",
    "            \n",
    "            normalized_mutual_info_score_sum += normalized_mutual_info_score(data[:, i],\n",
    "                                                                             data[:, j])\n",
    "            \n",
    "    return normalized_mutual_info_score_sum/(len(selected_bands)**2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1083184",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# rewards\n",
    "    \n",
    "path = r'/Users/romitbarua/Documents/Berkeley/Fall 2022/CS285-Deep Reinforcement Learning/HyperSpectralRL/data/data_indian_pines_drl.mat'\n",
    "num_iter = 50\n",
    "num_bands = 200\n",
    "bands_to_select = 30\n",
    "hyper_multiple = scipy.io.loadmat(path)['x'][:, :num_bands]\n",
    "\n",
    "    \n",
    "num_runs = 25\n",
    "\n",
    "\n",
    "correlations = []\n",
    "for i in range(num_runs):\n",
    "    correlations.append(calculate_correlations(hyper_multiple, num_bands_originally=hyper_multiple.shape[-1], num_bands_kept=30))\n",
    "print(f'\\nCorrelation reward for random 10 bands, x{num_runs} runs:', np.mean(correlations))\n",
    "\n",
    "# plot rewards\n",
    "a_string = ['pearson correlation (cumulative avg)'] * len(correlations)    \n",
    "#b_string = ['normalized mutual information (cumulative avg)'] * len(mis)\n",
    "#strings = a_string + b_string\n",
    "pd_df = pd.DataFrame([correlations, a_string]).T\n",
    "pd_df[0] = pd_df[0].astype(float, copy=True)\n",
    "sns.histplot(data=pd_df, bins=15, x=0, kde=True)\n",
    "plt.title(f'Test', fontsize=17)\n",
    "#plt.xlim([0,1])\n",
    "plt.show()\n",
    "plt.figure()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff98b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = [13, 24, 27, 28, 30, 37, 40, 47, 48, 53, 54, 66, 68, 69, 77, 78, 85, 86, 99, 137,\n",
    "       140, 147, 149, 157, 158, 161, 164, 169, 184, 194]\n",
    "\n",
    "def calculate_correlations(data):\n",
    "    \n",
    "    selected_bands = idx\n",
    "    corr_sum = 0\n",
    "    for i in selected_bands:\n",
    "        for j in selected_bands:\n",
    "            if i != j:\n",
    "            \n",
    "                corr_sum += np.abs(pearsonr(data[:, i], \n",
    "                                 data[:, j])[0])\n",
    "            \n",
    "    return corr_sum/(len(selected_bands)**2)\n",
    "\n",
    "calculate_correlations(hyper_multiple)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
