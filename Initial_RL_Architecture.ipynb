{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78720978",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "import h5py\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import accuracy_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import torch.nn as nn\n",
    "from scipy.stats.stats import pearsonr\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4cf4d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataManager():\n",
    "    \n",
    "    def __init__(self, params):\n",
    "        \n",
    "        self.rl_data = None\n",
    "        self.dataset_type = params['dataset_type']\n",
    "        self.data_file_path = params['data_file_path']\n",
    "        \n",
    "        #load the data\n",
    "        assert self.dataset_type in ('IndianPines', 'Botswana'), f'{self.dataset_type} is not valid' \n",
    "        #separating out in case any of the data requires unique pre-processig\n",
    "        if self.dataset_type == 'IndianPines':\n",
    "            self.load_indian_pine_data()\n",
    "        elif self.dataset_type == 'Botswana':\n",
    "            self.load_botswana_data()\n",
    "            \n",
    "        #self.x_train = None\n",
    "        #self.y_train = None\n",
    "        #self.x_test = None\n",
    "        #self.y_test = None\n",
    "\n",
    "    def load_indian_pine_data(self):\n",
    "        \n",
    "        self.rl_data = scipy.io.loadmat(self.data_file_path)['x']\n",
    "        #self.rl_data = h5py.File(self.data_file_path, 'r')\n",
    "        \n",
    "        #self.x_train = np.array(data['x_tra']).transpose()\n",
    "        #self.y_train = np.argmax(np.array(data['y_tra']).transpose(), axis=1)\n",
    "        #self.x_test = np.array(data['x_test']).transpose()\n",
    "        #self.y_test = np.argmax(np.array(data['y_test']).transpose(), axis=1)\n",
    "        \n",
    "    def load_botswana_data(self):\n",
    "        \n",
    "        self.rl_data = scipy.io.loadmat(self.data_file_path)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "433ba101",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer():\n",
    "    \n",
    "    def __init__(self, size=10000):\n",
    "        self.size = size\n",
    "        self.paths = []\n",
    "        \n",
    "    def add_trajectories(self, paths):\n",
    "        self.paths.extend(paths)\n",
    "        self.paths = self.paths[-self.size:]\n",
    "        \n",
    "    def sample_buffer_random(self, num_trajectories):\n",
    "        \n",
    "        rand_idx = np.random.permutation(len(self.paths))[:num_trajectories]\n",
    "        return [self.paths[i] for i in rand_idx]\n",
    "        #return self.paths[rand_idx]\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c496803",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    \n",
    "    def __init__(self, params):\n",
    "        \n",
    "        self.agent_params = params['agent']\n",
    "        self.n_iter = self.agent_params['n_iter']\n",
    "        self.trajectory_sample_size = self.agent_params['trajectory_sample_size']\n",
    "        self.batch_size = self.agent_params['batch_size']\n",
    "        self.num_critic_updates = self.agent_params['num_critic_updates']\n",
    "        \n",
    "        self.data_params = params['data']\n",
    "        self.DataManager = DataManager(self.data_params)\n",
    "        self.band_selection_num = self.data_params['band_selection_num']\n",
    "\n",
    "        self.critic_params = params['critic']\n",
    "        self.critic = QCritic(self.critic_params)\n",
    "        \n",
    "        \n",
    "        self.policy_params = params['policy']\n",
    "        self.policy = ArgMaxPolicy(self.policy_params, self.critic)\n",
    "        \n",
    "        self.replay_buffer = ReplayBuffer()\n",
    "        \n",
    "    \n",
    "    def generateTrajectories(self):\n",
    "        \n",
    "        #we expect paths to be a list of trajectories\n",
    "        #a trajectory is a list of Path objects\n",
    "        paths = []\n",
    "        for _ in range(self.trajectory_sample_size):\n",
    "            paths.append(self.sampleTrajectory())\n",
    "    \n",
    "        return paths\n",
    "    \n",
    "    def sampleTrajectory(self):\n",
    "            \n",
    "        #select 30 actions\n",
    "        state = np.zeros(200)\n",
    "        state_next = state.copy()\n",
    "\n",
    "        #paths will be a list of dictionary\n",
    "        path = []\n",
    "        for i in range(self.band_selection_num):\n",
    "            \n",
    "            action = self.policy.get_action(state)\n",
    "            state_next[action] = 1\n",
    "\n",
    "            reward = self.calculate_reward(state, state_next)\n",
    "            terminal = 1 if i == self.band_selection_num - 1 else 0\n",
    "            path.append(self.Path(state, action, state_next, reward, terminal))\n",
    "            \n",
    "            state = state_next.copy()\n",
    "\n",
    "        return path\n",
    "                   \n",
    "        \n",
    "    def runAgent(self):\n",
    "        \n",
    "        for iter_num in range(self.n_iter):\n",
    "            \n",
    "            print('Iteration ', iter_num, ':')\n",
    "            \n",
    "            paths = self.generateTrajectories()\n",
    "            self.replay_buffer.add_trajectories(paths)\n",
    "            \n",
    "            for _ in range(self.num_critic_updates):\n",
    "                sampled_paths = self.replay_buffer.sample_buffer_random(self.agent_params['batch_size'])\n",
    "                \n",
    "                flat_sampled_path = [path for trajectory in sampled_paths for path in trajectory]\n",
    "                obs = np.array([path['ob'] for path in flat_sampled_path])\n",
    "                acs = np.array([path['ac'] for path in flat_sampled_path])\n",
    "                obs_next = np.array([path['ob_next'] for path in flat_sampled_path])\n",
    "                res = np.array([path['re'] for path in flat_sampled_path])\n",
    "                terminals = np.array([path['terminal'] for path in flat_sampled_path])\n",
    "                \n",
    "                critic_loss = self.critic.update(obs, acs, obs_next, res, terminals)\n",
    "                \n",
    "            #sample a single trajectory\n",
    "            eval_path = self.sampleTrajectory()\n",
    "            print('Selected_Bands: ', np.argwhere(eval_path[-1]['ob']==1))\n",
    "            print('Eval_Return: ', np.sum(eval_path[-1]['re']))\n",
    "            print('Critic_Loss: ', critic_loss)\n",
    "                \n",
    "            \n",
    "            \n",
    "    def calculate_reward(self, state, state_next):\n",
    "        #for future, save down the previous state so that we can avoid a calc\n",
    "        return self.calculate_correlations(state) - self.calculate_correlations(state_next)\n",
    "    \n",
    "    \n",
    "    def calculate_correlations(self, state):\n",
    "        \n",
    "        #deal with the first state\n",
    "        ##### THIS LOGIC SEEMS WRONG - REGARDLESS OF THE FIRST PICK, YOU HAVE A REWARD OF 0#####\n",
    "        if np.sum(state) <= 1:\n",
    "            return 1\n",
    "        \n",
    "        selected_bands = np.squeeze(np.argwhere(np.array(state)==1))\n",
    "        corr_sum = 0\n",
    "        for i in selected_bands:\n",
    "            for j in selected_bands:\n",
    "                \n",
    "                corr_sum += pearsonr(self.DataManager.rl_data[:, i], self.DataManager.rl_data[:, j])[0]\n",
    "        return corr_sum/(len(selected_bands)**2)\n",
    "            \n",
    "            \n",
    "    def Path(self, ob, ac, ob_next, re, terminal):\n",
    "        return {'ob':ob,\n",
    "                'ac':ac,\n",
    "                'ob_next':ob_next,\n",
    "                're':re,\n",
    "                'terminal':terminal\n",
    "                }\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb12fe7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArgMaxPolicy():\n",
    "    \n",
    "    def __init__(self, params, critic):\n",
    "        self.epsilon = params['epsilon']\n",
    "        self.epsilon_decay = params['epsilon_decay']\n",
    "        self.critic = critic\n",
    "        \n",
    "    def get_action(self, obs):\n",
    "        \n",
    "        q_value_estimates = self.critic.get_action(obs)\n",
    "        \n",
    "        rand = np.random.rand()\n",
    "        if rand < self.epsilon:\n",
    "            #select a random action\n",
    "            unselected_bands = np.squeeze(np.argwhere(obs == 0))\n",
    "            selected_idx = np.random.choice(unselected_bands)\n",
    "        else:\n",
    "            selected_idx = torch.argmax(q_value_estimates)\n",
    "            \n",
    "        self.decay_epsilon()\n",
    "        return selected_idx\n",
    "                \n",
    "    def decay_epsilon(self):\n",
    "        self.epsilon *= self.epsilon_decay "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4bcd7fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QCritic():\n",
    "    \n",
    "    def __init__(self, params):\n",
    "        \n",
    "        self.critic = self.create_network()\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(),lr=0.005)\n",
    "        self.gamma = params['gamma']\n",
    "        \n",
    "        self.loss = nn.SmoothL1Loss()\n",
    "    \n",
    "    def create_network(self):\n",
    "        \n",
    "        q_net  = nn.Sequential(\n",
    "        nn.Linear(200, 400),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(400, 400),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(400, 200)\n",
    "        )\n",
    "        \n",
    "        return q_net\n",
    "    \n",
    "        \n",
    "    def forward(self, obs):\n",
    "        # will take in one hot encoded states and output a list of qu values\n",
    "        \n",
    "        q_values = self.critic(obs)\n",
    "        \n",
    "        return q_values\n",
    "    \n",
    "    def get_action(self, obs):\n",
    "        \n",
    "        if isinstance(obs, np.ndarray):\n",
    "            obs = from_numpy(obs)\n",
    "            \n",
    "        return self.critic(obs)\n",
    "    \n",
    "    def update(self, obs, ac_n, next_obs, reward_n, terminals):\n",
    "        \n",
    "        obs = self.check_tensor(obs)\n",
    "        ac_n = self.check_tensor(ac_n)\n",
    "        next_obs = self.check_tensor(next_obs)\n",
    "        reward_n = self.check_tensor(reward_n)\n",
    "        terminals = self.check_tensor(terminals)\n",
    "        \n",
    "        q_values = self.critic(obs)\n",
    "        \n",
    "        q_values_next = self.critic(next_obs)\n",
    "        \n",
    "        print('reward', reward_n.shape)\n",
    "        print('q_values_next', q_values_next.shape)\n",
    "        print('terminals', terminals.shape)\n",
    "        target = reward_n + self.gamma*q_values_next*(1-terminals)\n",
    "        target = target.detach()\n",
    "        \n",
    "        loss = self.loss(q_values, target)\n",
    "        \n",
    "        self.critic_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "        \n",
    "        return loss.item()\n",
    "    \n",
    "    def check_tensor(self, ar):\n",
    "        \n",
    "        if isinstance(ar, np.ndarray):\n",
    "            ar = from_numpy(ar)\n",
    "            \n",
    "        return ar\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "770adc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#utility functions\n",
    "## taken from Prof. Sergey Levine's CS285 HW\n",
    "\n",
    "device = 'cpu'\n",
    "\n",
    "def from_numpy(*args, **kwargs):\n",
    "    return torch.from_numpy(*args, **kwargs).float().to(device)\n",
    "\n",
    "def to_numpy(tensor):\n",
    "    return tensor.to('cpu').detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6dbabbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'agent':{\n",
    "            'n_iter':100,\n",
    "            'trajectory_sample_size':10,\n",
    "            'batch_size':10,\n",
    "            'num_critic_updates':5\n",
    "            },\n",
    "          'data':{\n",
    "            'band_selection_num':30,\n",
    "            'dataset_type':'IndianPines',\n",
    "            'data_file_path':'/Users/romitbarua/Documents/Berkeley/Fall 2022/CS285-Deep Reinforcement Learning/HyperSpectralRL/DRL4BS/data4drl/data_indian_pines_drl.mat' \n",
    "            },\n",
    "          'critic':{\n",
    "            'gamma':0.99\n",
    "            },\n",
    "          'policy':{\n",
    "            'epsilon':0.5,\n",
    "            'epsilon_decay':0.99\n",
    "            }\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3bfbd2c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  0 :\n",
      "reward torch.Size([300])\n",
      "q_values_next torch.Size([300, 200])\n",
      "terminals torch.Size([300])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (200) must match the size of tensor b (300) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-c9b7ddf1de76>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-90ec777c7add>\u001b[0m in \u001b[0;36mrunAgent\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     74\u001b[0m                 \u001b[0mterminals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'terminal'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mflat_sampled_path\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m                 \u001b[0mcritic_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobs_next\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;31m#sample a single trajectory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-175ee3508bb8>\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, obs, ac_n, next_obs, reward_n, terminals)\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'q_values_next'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_values_next\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'terminals'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreward_n\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mq_values_next\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mterminals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (200) must match the size of tensor b (300) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "agent = Agent(params)\n",
    "agent.runAgent()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
