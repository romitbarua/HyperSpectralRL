{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0d98917",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "import h5py\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import accuracy_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import torch.nn as nn\n",
    "from scipy.stats.stats import pearsonr\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94392f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataManager():\n",
    "    \n",
    "    def __init__(self, params):\n",
    "        \n",
    "        self.rl_data = None\n",
    "        self.dataset_type = params['dataset_type']\n",
    "        self.data_file_path = params['data_file_path']\n",
    "        \n",
    "        #load the data\n",
    "        assert self.dataset_type in ('IndianPines', 'Botswana'), f'{self.dataset_type} is not valid' \n",
    "        #separating out in case any of the data requires unique pre-processig\n",
    "        if self.dataset_type == 'IndianPines':\n",
    "            self.load_indian_pine_data()\n",
    "        elif self.dataset_type == 'Botswana':\n",
    "            self.load_botswana_data()\n",
    "            \n",
    "        #self.x_train = None\n",
    "        #self.y_train = None\n",
    "        #self.x_test = None\n",
    "        #self.y_test = None\n",
    "\n",
    "    def load_indian_pine_data(self):\n",
    "        \n",
    "        #self.rl_data = scipy.io.loadmat(self.data_file_path)\n",
    "        self.rl_data = h5py.File(self.data_file_path, 'r')\n",
    "        \n",
    "        #self.x_train = np.array(data['x_tra']).transpose()\n",
    "        #self.y_train = np.argmax(np.array(data['y_tra']).transpose(), axis=1)\n",
    "        #self.x_test = np.array(data['x_test']).transpose()\n",
    "        #self.y_test = np.argmax(np.array(data['y_test']).transpose(), axis=1)\n",
    "        \n",
    "    def load_botswana_data(self):\n",
    "        \n",
    "        self.rl_data = scipy.io.loadmat(self.data_file_path)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "300b016d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer():\n",
    "    \n",
    "    def __init__(self, size=10000):\n",
    "        self.size = size\n",
    "        self.paths = []\n",
    "        \n",
    "    def add_trajectories(self, paths):\n",
    "        self.paths.extend(paths)\n",
    "        self.paths = self.paths[-self.max_size:]\n",
    "        \n",
    "    def sample_buffer_random(self, num_trajectories):\n",
    "        \n",
    "        rand_idx = np.random.permutation(len(self.paths))[:num_trajectories]\n",
    "        return self.paths[rand_idx]\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b151ee43",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    \n",
    "    def __init__(self, params):\n",
    "        \n",
    "        self.agent_params = params['agent']\n",
    "        self.n_iter = self.agent_params['n_iter']\n",
    "        self.trajectory_sample_size = self.agent_params['trajectory_sample_size']\n",
    "        self.batch_size = self.agent_params['batch_size']\n",
    "        self.num_critic_updates = self.agent_params['num_critic_updates']\n",
    "        \n",
    "        self.data_params = params['data']\n",
    "        self.DataManager = DataManager(self.data_params)\n",
    "        self.band_selection_num = self.data_params['band_selection_num']\n",
    "\n",
    "        self.critic_params = params['critic']\n",
    "        self.critic = QCritic(self.critic_params)\n",
    "        \n",
    "        self.policy = ArgMaxPolicy(self.critic)\n",
    "        \n",
    "        self.replay_buffer = ReplayBuffer()\n",
    "        \n",
    "    \n",
    "    def generateTrajectories(self):\n",
    "        \n",
    "        #we expect paths to be a list of trajectories\n",
    "        #a trajectory is a list of Path objects\n",
    "        paths = []\n",
    "        for _ in range(self.trajectory_sample_size):\n",
    "            paths.append(self.sampleTrajectory())\n",
    "    \n",
    "        return paths\n",
    "    \n",
    "    def sampleTrajectory(self):\n",
    "            \n",
    "        #select 30 actions\n",
    "        state = np.zeros(200)\n",
    "        state_next = state\n",
    "\n",
    "        #paths will be a list of dictionary\n",
    "        path = []\n",
    "        for i in range(self.band_selection_num):\n",
    "            \n",
    "            action = self.policy.get_action(state)\n",
    "            state_next[action] = 1\n",
    "\n",
    "            reward = calculate_rewards(state, state_next)\n",
    "            terminal = 1 if i == 29 else 0\n",
    "            path.append(Path(state, action, state_next, reward, terminal))\n",
    "\n",
    "        return path\n",
    "                   \n",
    "        \n",
    "    def runAgent(self):\n",
    "        \n",
    "        for iter_num in range(self.n_iter):\n",
    "            \n",
    "            print('Iteration ', iter_num, ':')\n",
    "            \n",
    "            paths = self.generateTrajectories()\n",
    "            self.replay_buffer.add_trajectories(paths)\n",
    "            \n",
    "            for _ in range(self.num_critic_updates):\n",
    "                sampled_paths = self.replay_buffer.sample_buffer_random(self.agent_params['batch_size'])\n",
    "                \n",
    "                flat_sampled_path = [path for trajectory in sampled_paths for path in trajectory]\n",
    "                obs = [path['ob'] for path in flat_sampled_path]\n",
    "                acs = [path['ac'] for path in flat_sampled_path]\n",
    "                obs_next = [path['ob_next'] for path in flat_sampled_path]\n",
    "                res = [path['re'] for path in flat_sampled_path]\n",
    "                terminals = [path['terminal'] for path in flat_sampled_path]\n",
    "                \n",
    "                self.critic.update(obs, acs, obs_next, res, terminals)\n",
    "            \n",
    "            \n",
    "    def calculate_reward(state, state_next):\n",
    "        #for future, save down the previous state so that we can avoid a calc\n",
    "        return self.calculate_correation(state) - self.calculate_correation(state_next)\n",
    "    \n",
    "    \n",
    "    def calculate_correlations(state):\n",
    "        \n",
    "        selected_bands = np.squeeze(np.argwhere(np.array(state)==1))\n",
    "        corr_sum = 0\n",
    "        for i in selected_bands:\n",
    "            for j in selected_bands:\n",
    "                corr_sum += pearsonr(self.DataManger.rl_data[:, i], self.DataManger.rl_data[:, i])\n",
    "        return corr_sum/(len(selected_bands)**2)\n",
    "            \n",
    "            \n",
    "    def Path(ob, ac, ob_next, re, terminal):\n",
    "        return {'ob':ob,\n",
    "                'ac':ac,\n",
    "                'ob_next':ob_next,\n",
    "                're':re,\n",
    "                'terminal':terminal\n",
    "                }\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ca8508c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArgMaxPolicy():\n",
    "    \n",
    "    def __init__(self, critic):\n",
    "        self.critic = critic\n",
    "        \n",
    "    def get_action(self, obs):\n",
    "        \n",
    "        \n",
    "        q_value_estimates = self.critic.get_action(obs)\n",
    "        \n",
    "        #return index of best action\n",
    "        return np.argmax(q_value_estimates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2402fb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QCritic():\n",
    "    \n",
    "    def __init__(self, params):\n",
    "        \n",
    "        self.critic = self.create_network()\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(),lr=0.005)\n",
    "        self.gamma = params['gamma']\n",
    "        \n",
    "        self.loss = nn.SmoothL1Loss()\n",
    "    \n",
    "    def create_network(self):\n",
    "        \n",
    "        q_net  = nn.Sequential(\n",
    "        nn.Linear(200, 400),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(400, 400),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(400, 200)\n",
    "        )\n",
    "        \n",
    "        return q_net\n",
    "    \n",
    "        \n",
    "    def forward(self, obs):\n",
    "        # will take in one hot encoded states and output a list of qu values\n",
    "        \n",
    "        q_values = self.critic(obs)\n",
    "        \n",
    "        return q_values\n",
    "    \n",
    "    def get_action(self, obs):\n",
    "        \n",
    "        \n",
    "        return self.critic(obs)\n",
    "    \n",
    "    def update(self, obs, ac_n, next_obs, reward_n):\n",
    "        \n",
    "        q_values = self.critic(obs)\n",
    "        \n",
    "        q_values_next = self.critic(next_obs)\n",
    "        \n",
    "        target = reward_n + self.gamma*q_values_next\n",
    "        target = target.detach()\n",
    "        \n",
    "        loss = self.loss(q_values, target)\n",
    "        \n",
    "        self.critic_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "        \n",
    "        return loss.item()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec35edd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#utility functions\n",
    "## taken from Prof. Sergey Levine's CS285 HW\n",
    "def from_numpy(*args, **kwargs):\n",
    "    return torch.from_numpy(*args, **kwargs).float().to(device)\n",
    "\n",
    "def to_numpy(tensor):\n",
    "    return tensor.to('cpu').detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22515839",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'agent':{\n",
    "            'n_iter':100,\n",
    "            'trajectory_sample_size':100,\n",
    "            'batch_size':50,\n",
    "            'num_critic_updates':10\n",
    "            },\n",
    "          'data':{\n",
    "            'band_selection_num':30,\n",
    "            'dataset_type':'IndianPines',\n",
    "            'data_file_path':'/Users/romitbarua/Documents/Berkeley/Fall 2022/CS285-Deep Reinforcement Learning/Final Project/DRL4BS/data4classification/indian_pines_randomSampling_0.1_run_1.mat' \n",
    "            },\n",
    "          'critic':{\n",
    "            'gamma':0.99\n",
    "            }\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5be1c2a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  0 :\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "linear(): argument 'input' (position 1) must be Tensor, not numpy.ndarray",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-c9b7ddf1de76>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-98d4fbe83494>\u001b[0m in \u001b[0;36mrunAgent\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Iteration '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miter_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m':'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m             \u001b[0mpaths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerateTrajectories\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_trajectories\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpaths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-98d4fbe83494>\u001b[0m in \u001b[0;36mgenerateTrajectories\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mpaths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrajectory_sample_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0mpaths\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msampleTrajectory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpaths\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-98d4fbe83494>\u001b[0m in \u001b[0;36msampleTrajectory\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mband_selection_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m             \u001b[0mstate_next\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-c8e65611a28b>\u001b[0m in \u001b[0;36mget_action\u001b[0;34m(self, obs)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mq_value_estimates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;31m#return index of best action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-79f132f78d45>\u001b[0m in \u001b[0;36mget_action\u001b[0;34m(self, obs)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mac_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_obs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: linear(): argument 'input' (position 1) must be Tensor, not numpy.ndarray"
     ]
    }
   ],
   "source": [
    "agent = Agent(params)\n",
    "agent.runAgent()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
